---
created: 2025-04-23T01:14:03+03:00
modified: 2025-04-23T01:18:32+03:00
category:
  - "[[Программирование]]"
source: https://commoncrawl.org/
---

# Common Crawl API: Бесплатная база данных для веб-скрейпинга и анализа интернета

[Common Crawl](https://commoncrawl.org/) — не просто API, а целый архив интернета, содержащий огромные объемы веб-данных, собранных с 2008 года. В отличие от стандартных API для веб-скрейпинга, Common Crawl предоставляет доступ к готовым копиям страниц, что значительно ускоряет анализ веб-контента и снижает нагрузку на исходные сайты.

## Как Common Crawl API экономит время?

Вместо того чтобы разрабатывать сложные парсеры и загружать миллионы страниц вручную, Common Crawl позволяет быстро находить нужную информацию в готовых архивах:

- Бесплатный доступ к огромной базе веб-страниц (петабайты данных, обновляемых ежемесячно).

- Исторические данные (можно анализировать, как изменялся контент сайтов за годы).

- Отсутствие блокировок и капч (данные уже собраны, вам не нужно бороться с защитами сайтов).

- Возможность массового анализа веба (идеально для NLP, машинного обучения и SEO-исследований).

API и данные Common Crawl полезны для исследователей, дата-аналитиков, SEO-специалистов и разработчиков.

## Как использовать Common Crawl API?

Common Crawl предоставляет данные в формате WARC (архивные копии страниц) и WET (чистый текст без HTML). Доступ осуществляется через Amazon S3, но также можно использовать API Common Crawl Index для поиска нужных URL.

### Поиск веб-страниц через API

Допустим, нам нужны все страницы, содержащие example.com:

```
curl -X GET "http://index.commoncrawl.org/CC-MAIN-2023-50-index?url=example.com&output=json"
```

Вот такой ответ может быть:

```
[
  {
    "url": "http://example.com/",
    "timestamp": "20231201094512",
    "status": "200",
    "length": "1278",
    "mime": "text/html",
    "digest": "SHA256:abcd1234...",
    "offset": "67890",
    "filename": "crawl-data/CC-MAIN-2023-50/segments/.../warc.gz"
  }
]
```

### Получение текста страницы из архива

После получения ссылки на WARC-файл можно скачать его и распаковать:

```
wget https://data.commoncrawl.org/crawl-data/CC-MAIN-2023-50/segments/.../warc.gz
gunzip warc.gz
```

Ну и затем извлечь текст:

```
warc2text warc > output.txt
```

### Анализ больших объемов данных с AWS

Если вам нужны миллионы страниц, можно использовать AWS Athena для обработки данных прямо в облаке.

Пример SQL-запроса в AWS Athena для поиска страниц с «machine learning»:

```
SELECT url, content 
FROM commoncrawl
WHERE content LIKE '%machine learning%'
LIMIT 100;
```

> Важно отметить, что данные предоставляются в сыром виде и их нужно дополнительно обрабатывать. Плюс нет гарантии, что конкретная страница будет в архиве.
